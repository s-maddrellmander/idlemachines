{
  "checkpoint": "checkpoints/checkpoint_fp8_1.5B_20260102_07182.pt",
  "checkpoint_name": "checkpoint_fp8_1.5B_20260102_07182",
  "step": 15000,
  "val_loss": 2.137849807739258,
  "train_loss": 2.1285412311553955,
  "model_config": {
    "n_layer": 24,
    "n_embd": 2048,
    "n_head": 16,
    "n_kv_head": 4
  },
  "training_args": {
    "steps": 15000,
    "batch_size": 64,
    "grad_accum": 4,
    "seq_len": 1024,
    "lr": 0.0003,
    "min_lr": 3e-05,
    "warmup_frac": 0.02,
    "warmup_steps": 300,
    "weight_decay": 0.1,
    "grad_clip": 1.0,
    "model_size": "1.5B",
    "train_data": "data/pretrain/train_*.bin",
    "val_data": "data/pretrain/val_*.bin",
    "val_every": 500,
    "val_batches": 50,
    "log_every": 10,
    "log_file": "experiments/1.5B_fp8_20260102_071825/train_fp8_1.5B.csv",
    "checkpoint_path": "experiments/1.5B_fp8_20260102_071825/checkpoint_fp8_1.5B.pt",
    "resume": null,
    "fp8_recipe": "tensorwise",
    "no_fp8": false,
    "use_fp8": true
  },
  "eval_config": {
    "device": "auto",
    "dtype": "bfloat16",
    "batch_size": 8,
    "num_fewshot": 0,
    "limit": 100
  },
  "timestamp": "2026-01-02T23:41:12.231027",
  "tasks": [
    "hellaswag",
    "arc_easy",
    "arc_challenge",
    "piqa",
    "winogrande",
    "lambada_openai",
    "boolq",
    "mmlu"
  ],
  "scores": {
    "arc_challenge": 0.27,
    "arc_easy": 0.55,
    "boolq": 0.49,
    "hellaswag": 0.5,
    "lambada_openai": 0.3,
    "mmlu": 0.2424561403508772,
    "mmlu_humanities": 0.2530769230769231,
    "mmlu_formal_logic": 0.28,
    "mmlu_high_school_european_history": 0.25,
    "mmlu_high_school_us_history": 0.2,
    "mmlu_high_school_world_history": 0.27,
    "mmlu_international_law": 0.24,
    "mmlu_jurisprudence": 0.24,
    "mmlu_logical_fallacies": 0.19,
    "mmlu_moral_disputes": 0.27,
    "mmlu_moral_scenarios": 0.24,
    "mmlu_philosophy": 0.18,
    "mmlu_prehistory": 0.33,
    "mmlu_professional_law": 0.31,
    "mmlu_world_religions": 0.29,
    "mmlu_other": 0.23307692307692307,
    "mmlu_business_ethics": 0.34,
    "mmlu_clinical_knowledge": 0.14,
    "mmlu_college_medicine": 0.19,
    "mmlu_global_facts": 0.32,
    "mmlu_human_aging": 0.15,
    "mmlu_management": 0.26,
    "mmlu_marketing": 0.31,
    "mmlu_medical_genetics": 0.24,
    "mmlu_miscellaneous": 0.23,
    "mmlu_nutrition": 0.22,
    "mmlu_professional_accounting": 0.2,
    "mmlu_professional_medicine": 0.18,
    "mmlu_virology": 0.25,
    "mmlu_social_sciences": 0.23166666666666666,
    "mmlu_econometrics": 0.29,
    "mmlu_high_school_geography": 0.18,
    "mmlu_high_school_government_and_politics": 0.2,
    "mmlu_high_school_macroeconomics": 0.16,
    "mmlu_high_school_microeconomics": 0.23,
    "mmlu_high_school_psychology": 0.2,
    "mmlu_human_sexuality": 0.32,
    "mmlu_professional_psychology": 0.19,
    "mmlu_public_relations": 0.23,
    "mmlu_security_studies": 0.27,
    "mmlu_sociology": 0.22,
    "mmlu_us_foreign_policy": 0.29,
    "mmlu_stem": 0.24842105263157896,
    "mmlu_abstract_algebra": 0.23,
    "mmlu_anatomy": 0.25,
    "mmlu_astronomy": 0.28,
    "mmlu_college_biology": 0.25,
    "mmlu_college_chemistry": 0.2,
    "mmlu_college_computer_science": 0.23,
    "mmlu_college_mathematics": 0.24,
    "mmlu_college_physics": 0.17,
    "mmlu_computer_security": 0.33,
    "mmlu_conceptual_physics": 0.25,
    "mmlu_electrical_engineering": 0.28,
    "mmlu_elementary_mathematics": 0.23,
    "mmlu_high_school_biology": 0.19,
    "mmlu_high_school_chemistry": 0.22,
    "mmlu_high_school_computer_science": 0.28,
    "mmlu_high_school_mathematics": 0.27,
    "mmlu_high_school_physics": 0.29,
    "mmlu_high_school_statistics": 0.24,
    "mmlu_machine_learning": 0.29,
    "piqa": 0.71,
    "winogrande": 0.48
  },
  "mean_accuracy": 0.265633300084101,
  "full_results": {
    "arc_challenge": {
      "alias": "arc_challenge",
      "acc,none": 0.3,
      "acc_stderr,none": 0.04605661864718382,
      "acc_norm,none": 0.27,
      "acc_norm_stderr,none": 0.04461960433384737
    },
    "arc_easy": {
      "alias": "arc_easy",
      "acc,none": 0.63,
      "acc_stderr,none": 0.048523658709390974,
      "acc_norm,none": 0.55,
      "acc_norm_stderr,none": 0.05
    },
    "boolq": {
      "alias": "boolq",
      "acc,none": 0.49,
      "acc_stderr,none": 0.05024183937956913
    },
    "hellaswag": {
      "alias": "hellaswag",
      "acc,none": 0.4,
      "acc_stderr,none": 0.0492365963917331,
      "acc_norm,none": 0.5,
      "acc_norm_stderr,none": 0.050251890762960605
    },
    "lambada_openai": {
      "alias": "lambada_openai",
      "perplexity,none": 45.62348089376112,
      "perplexity_stderr,none": 14.791620348572064,
      "acc,none": 0.3,
      "acc_stderr,none": 0.04605661864718382
    },
    "mmlu": {
      "acc,none": 0.2424561403508772,
      "acc_stderr,none": 0.005669088120359345,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "acc,none": 0.2530769230769231,
      "acc_stderr,none": 0.012058719125643838,
      "alias": " - humanities"
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.28,
      "acc_stderr,none": 0.045126085985421296
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.27,
      "acc_stderr,none": 0.04461960433384737
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.24,
      "acc_stderr,none": 0.04292346959909278
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.24,
      "acc_stderr,none": 0.04292346959909278
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.19,
      "acc_stderr,none": 0.039427724440366255
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.27,
      "acc_stderr,none": 0.04461960433384737
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.24,
      "acc_stderr,none": 0.04292346959909278
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.18,
      "acc_stderr,none": 0.03861229196653691
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.33,
      "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117317
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.29,
      "acc_stderr,none": 0.045604802157206865
    },
    "mmlu_other": {
      "acc,none": 0.23307692307692307,
      "acc_stderr,none": 0.011664168896789692,
      "alias": " - other"
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.34,
      "acc_stderr,none": 0.04760952285695233
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.14,
      "acc_stderr,none": 0.0348735088019777
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.19,
      "acc_stderr,none": 0.039427724440366255
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.32,
      "acc_stderr,none": 0.04688261722621507
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.15,
      "acc_stderr,none": 0.035887028128263665
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.26,
      "acc_stderr,none": 0.0440844002276808
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117317
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.24,
      "acc_stderr,none": 0.04292346959909278
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.22,
      "acc_stderr,none": 0.041633319989322654
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.18,
      "acc_stderr,none": 0.03861229196653691
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_social_sciences": {
      "acc,none": 0.23166666666666666,
      "acc_stderr,none": 0.012160657618522407,
      "alias": " - social sciences"
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.29,
      "acc_stderr,none": 0.045604802157206865
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.18,
      "acc_stderr,none": 0.03861229196653691
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.16,
      "acc_stderr,none": 0.03684529491774706
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.32,
      "acc_stderr,none": 0.04688261722621507
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.19,
      "acc_stderr,none": 0.039427724440366255
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.27,
      "acc_stderr,none": 0.04461960433384737
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.22,
      "acc_stderr,none": 0.041633319989322654
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.29,
      "acc_stderr,none": 0.045604802157206865
    },
    "mmlu_stem": {
      "acc,none": 0.24842105263157896,
      "acc_stderr,none": 0.009924305700863825,
      "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.28,
      "acc_stderr,none": 0.045126085985421296
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.2,
      "acc_stderr,none": 0.04020151261036849
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.24,
      "acc_stderr,none": 0.04292346959909278
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.17,
      "acc_stderr,none": 0.03775251680686369
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.33,
      "acc_stderr,none": 0.04725815626252609
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.28,
      "acc_stderr,none": 0.045126085985421296
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816507
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.19,
      "acc_stderr,none": 0.039427724440366255
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.22,
      "acc_stderr,none": 0.041633319989322654
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.28,
      "acc_stderr,none": 0.045126085985421296
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.27,
      "acc_stderr,none": 0.04461960433384737
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.29,
      "acc_stderr,none": 0.045604802157206865
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.24,
      "acc_stderr,none": 0.04292346959909278
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.29,
      "acc_stderr,none": 0.045604802157206865
    },
    "piqa": {
      "alias": "piqa",
      "acc,none": 0.72,
      "acc_stderr,none": 0.045126085985421296,
      "acc_norm,none": 0.71,
      "acc_norm_stderr,none": 0.045604802157206865
    },
    "winogrande": {
      "alias": "winogrande",
      "acc,none": 0.48,
      "acc_stderr,none": 0.05021167315686783
    }
  }
}