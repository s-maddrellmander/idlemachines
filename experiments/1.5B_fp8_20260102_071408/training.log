W0102 07:14:09.324000 123324 site-packages/torch/distributed/run.py:803] 
W0102 07:14:09.324000 123324 site-packages/torch/distributed/run.py:803] *****************************************
W0102 07:14:09.324000 123324 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0102 07:14:09.324000 123324 site-packages/torch/distributed/run.py:803] *****************************************
======================================================================
FP8 TRAINING - OPTIMIZED FOR THROUGHPUT
======================================================================

------------------Configuration-------------------
Device: NVIDIA H200 x 8
DDP: True (rank 0/8)
Mode: fp8_tensorwise
Steps: 15000 (optimizer steps)
Micro batch size: 64 per GPU
Gradient accumulation: 4
Effective batch size: 2048 sequences
Sequence length: 1024
Tokens per step: 2,097,152
Learning rate: 0.0003 (warmup: 300 steps, min: 3e-05)
Validation every: 500 steps
Model preset: 1.5B

----------------------Model-----------------------
Parameters: 1.56B (1562.7M)
Layers: 24
Heads: 16 (KV: 4)
Embed dim: 2048

------------------FP8 Conversion------------------
Converted 169/169 Linear layers to Float8Linear
NCCL version 2.27.7+cuda13.0

-------------------Compilation--------------------
Compiling model with torch.compile()...

-----------------------Data-----------------------
Data: 499 shards, ~48.85B tokens total
Data: 1 shards, ~0.10B tokens total
Train data: data/pretrain/train_*.bin
Val data: data/pretrain/val_*.bin
Tokens per optimizer step: 2,097,152
Total tokens: 31457.3M (31.46B)

---------------------Logging----------------------
Log file: experiments/1.5B_fp8_20260102_071408/train_fp8_1.5B.csv
Checkpoint: experiments/1.5B_fp8_20260102_071408/checkpoint_fp8_1.5B.pt

======================================================================
Starting training...
======================================================================

[rank3]:W0102 07:14:36.443000 123432 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank7]:W0102 07:14:36.444000 123436 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0102 07:14:36.445000 123429 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:W0102 07:14:36.449000 123435 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0102 07:14:36.450000 123430 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0102 07:14:36.486000 123434 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0102 07:14:36.489000 123433 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0102 07:14:36.499000 123431 site-packages/torch/_logging/_internal.py:1199] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
step      1/15000 | loss: 10.7564 | ppl: 46931.3 | lr: 1.00e-06 | grad: 10.83 | tok/s: 63624 | tokens: 2.1M | train_time: 33.0s | wall_time: 33.0s
step     10/15000 | loss: 9.9926 | ppl: 21864.7 | lr: 1.00e-05 | grad: 4.50 | tok/s: 576752 | tokens: 21.0M | train_time: 1.1m | wall_time: 1.1m
step     20/15000 | loss: 9.4974 | ppl: 13325.7 | lr: 2.00e-05 | grad: 4.43 | tok/s: 567603 | tokens: 41.9M | train_time: 1.7m | wall_time: 1.7m
step     30/15000 | loss: 8.9812 | ppl: 7952.2 | lr: 3.00e-05 | grad: 4.70 | tok/s: 569626 | tokens: 62.9M | train_time: 2.3m | wall_time: 2.3m
step     40/15000 | loss: 8.2908 | ppl: 3987.0 | lr: 4.00e-05 | grad: 4.51 | tok/s: 568332 | tokens: 83.9M | train_time: 2.9m | wall_time: 2.9m
