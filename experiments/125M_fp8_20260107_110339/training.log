======================================================================
FP8 TRAINING - DDP MULTI-GPU
======================================================================

------------------Configuration-------------------
Device: NVIDIA H100 80GB HBM3 x 1
DDP: True (rank 0/1)
Mode: fp8_tensorwise
Steps: 1335 (optimizer steps)
Micro batch size: 64 per GPU
Gradient accumulation: 32
Effective batch size: 2048 sequences
Sequence length: 1024
Tokens per step: 2,097,152
Learning rate: 0.0003 (warmup: 26 steps, min: 3e-05)
Validation every: 500 steps
Model preset: 125M

----------------------Model-----------------------
Parameters: 0.14B (142.5M)
Layers: 12
Heads: 12 (KV: 4)
Embed dim: 768

------------------FP8 Conversion------------------
Converted 61/85 Linear layers to Float8Linear
NCCL version 2.21.5+cuda12.4

-------------------Compilation--------------------
Compiling model with torch.compile()...

-----------------------Data-----------------------
Data: 100 shards, ~10.00B tokens total
Data: 1 shards, ~0.10B tokens total
Train data: data/pretrain/train_*.bin
Val data: data/pretrain/val_*.bin
Tokens per optimizer step: 2,097,152
Total tokens: 2799.7M (2.80B)

---------------------Logging----------------------
Log file: experiments/125M_fp8_20260107_110339/train_fp8_125M.csv
Checkpoint: experiments/125M_fp8_20260107_110339/checkpoint_fp8_125M.pt

======================================================================
Starting training...
======================================================================

[rank0]:W0107 11:03:52.162000 5393 site-packages/torch/_logging/_internal.py:1089] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
step      1/1335 | loss: 10.7340 | ppl: 45890.8 | lr: 1.15e-05 | grad: 4.38 | tok/s: 97626 | tokens: 2.1M | train_time: 21.5s | wall_time: 21.5s
step     10/1335 | loss: 9.9991 | ppl: 22007.0 | lr: 1.15e-04 | grad: 3.00 | tok/s: 385412 | tokens: 21.0M | train_time: 1.2m | wall_time: 1.2m
step     20/1335 | loss: 8.7430 | ppl: 6266.5 | lr: 2.31e-04 | grad: 2.98 | tok/s: 377041 | tokens: 41.9M | train_time: 2.1m | wall_time: 2.1m
step     30/1335 | loss: 7.5177 | ppl: 1840.3 | lr: 3.00e-04 | grad: 2.77 | tok/s: 369621 | tokens: 62.9M | train_time: 3.0m | wall_time: 3.0m
step     40/1335 | loss: 6.9882 | ppl: 1083.8 | lr: 3.00e-04 | grad: 7.19 | tok/s: 364049 | tokens: 83.9M | train_time: 4.0m | wall_time: 4.0m
step     50/1335 | loss: 6.7403 | ppl: 845.8 | lr: 3.00e-04 | grad: 2.17 | tok/s: 382912 | tokens: 104.9M | train_time: 4.9m | wall_time: 4.9m
step     60/1335 | loss: 6.4377 | ppl: 624.9 | lr: 3.00e-04 | grad: 0.47 | tok/s: 377044 | tokens: 125.8M | train_time: 5.8m | wall_time: 5.8m
step     70/1335 | loss: 6.2362 | ppl: 510.9 | lr: 2.99e-04 | grad: 0.49 | tok/s: 377701 | tokens: 146.8M | train_time: 6.7m | wall_time: 6.7m
step     80/1335 | loss: 6.0772 | ppl: 435.8 | lr: 2.99e-04 | grad: 0.51 | tok/s: 372522 | tokens: 167.8M | train_time: 7.7m | wall_time: 7.7m
step     90/1335 | loss: 6.0432 | ppl: 421.2 | lr: 2.98e-04 | grad: 0.37 | tok/s: 375818 | tokens: 188.7M | train_time: 8.6m | wall_time: 8.6m
step    100/1335 | loss: 5.9495 | ppl: 383.6 | lr: 2.98e-04 | grad: 0.69 | tok/s: 367800 | tokens: 209.7M | train_time: 9.5m | wall_time: 9.5m
step    110/1335 | loss: 5.8412 | ppl: 344.2 | lr: 2.97e-04 | grad: 0.42 | tok/s: 385410 | tokens: 230.7M | train_time: 10.5m | wall_time: 10.5m
step    120/1335 | loss: 5.7482 | ppl: 313.6 | lr: 2.97e-04 | grad: 0.46 | tok/s: 365061 | tokens: 251.7M | train_time: 11.4m | wall_time: 11.4m
step    130/1335 | loss: 5.6390 | ppl: 281.2 | lr: 2.96e-04 | grad: 1.21 | tok/s: 374574 | tokens: 272.6M | train_time: 12.3m | wall_time: 12.3m
step    140/1335 | loss: 5.5964 | ppl: 269.4 | lr: 2.95e-04 | grad: 0.38 | tok/s: 375794 | tokens: 293.6M | train_time: 13.3m | wall_time: 13.3m
step    150/1335 | loss: 5.6000 | ppl: 270.4 | lr: 2.94e-04 | grad: 0.57 | tok/s: 373692 | tokens: 314.6M | train_time: 14.2m | wall_time: 14.2m
step    160/1335 | loss: 5.4010 | ppl: 221.6 | lr: 2.93e-04 | grad: 0.57 | tok/s: 374450 | tokens: 335.5M | train_time: 15.2m | wall_time: 15.2m
step    170/1335 | loss: 5.3650 | ppl: 213.8 | lr: 2.92e-04 | grad: 0.58 | tok/s: 365221 | tokens: 356.5M | train_time: 16.1m | wall_time: 16.1m
step    180/1335 | loss: 5.3009 | ppl: 200.5 | lr: 2.91e-04 | grad: 0.62 | tok/s: 370754 | tokens: 377.5M | train_time: 17.0m | wall_time: 17.0m
step    190/1335 | loss: 5.2092 | ppl: 183.0 | lr: 2.90e-04 | grad: 0.44 | tok/s: 385809 | tokens: 398.5M | train_time: 18.0m | wall_time: 18.0m
step    200/1335 | loss: 5.3078 | ppl: 201.9 | lr: 2.88e-04 | grad: 0.85 | tok/s: 381142 | tokens: 419.4M | train_time: 18.9m | wall_time: 18.9m
step    210/1335 | loss: 5.1439 | ppl: 171.4 | lr: 2.87e-04 | grad: 0.52 | tok/s: 390062 | tokens: 440.4M | train_time: 19.8m | wall_time: 19.8m
step    220/1335 | loss: 5.0975 | ppl: 163.6 | lr: 2.86e-04 | grad: 1.09 | tok/s: 367583 | tokens: 461.4M | train_time: 20.8m | wall_time: 20.8m
step    230/1335 | loss: 5.1710 | ppl: 176.1 | lr: 2.84e-04 | grad: 0.75 | tok/s: 378401 | tokens: 482.3M | train_time: 21.7m | wall_time: 21.7m
step    240/1335 | loss: 5.0147 | ppl: 150.6 | lr: 2.83e-04 | grad: 0.53 | tok/s: 371579 | tokens: 503.3M | train_time: 22.6m | wall_time: 22.6m
step    250/1335 | loss: 4.9361 | ppl: 139.2 | lr: 2.81e-04 | grad: 0.97 | tok/s: 360225 | tokens: 524.3M | train_time: 23.6m | wall_time: 23.6m
step    260/1335 | loss: 4.8486 | ppl: 127.6 | lr: 2.79e-04 | grad: 0.53 | tok/s: 376173 | tokens: 545.3M | train_time: 24.5m | wall_time: 24.5m
step    270/1335 | loss: 4.8304 | ppl: 125.3 | lr: 2.78e-04 | grad: 0.77 | tok/s: 378718 | tokens: 566.2M | train_time: 25.4m | wall_time: 25.4m
step    280/1335 | loss: 4.8308 | ppl: 125.3 | lr: 2.76e-04 | grad: 0.47 | tok/s: 374523 | tokens: 587.2M | train_time: 26.4m | wall_time: 26.4m
step    290/1335 | loss: 4.8173 | ppl: 123.6 | lr: 2.74e-04 | grad: 0.76 | tok/s: 378581 | tokens: 608.2M | train_time: 27.3m | wall_time: 27.3m
step    300/1335 | loss: 4.7316 | ppl: 113.5 | lr: 2.72e-04 | grad: 0.70 | tok/s: 381387 | tokens: 629.1M | train_time: 28.2m | wall_time: 28.2m
step    310/1335 | loss: 4.5778 | ppl: 97.3 | lr: 2.70e-04 | grad: 0.54 | tok/s: 378459 | tokens: 650.1M | train_time: 29.1m | wall_time: 29.1m
step    320/1335 | loss: 4.6027 | ppl: 99.8 | lr: 2.68e-04 | grad: 0.65 | tok/s: 387964 | tokens: 671.1M | train_time: 30.0m | wall_time: 30.0m
step    330/1335 | loss: 4.5701 | ppl: 96.6 | lr: 2.66e-04 | grad: 0.72 | tok/s: 380776 | tokens: 692.1M | train_time: 31.0m | wall_time: 31.0m
step    340/1335 | loss: 4.6016 | ppl: 99.6 | lr: 2.63e-04 | grad: 0.55 | tok/s: 377083 | tokens: 713.0M | train_time: 31.9m | wall_time: 31.9m
step    350/1335 | loss: 4.6866 | ppl: 108.5 | lr: 2.61e-04 | grad: 0.73 | tok/s: 379831 | tokens: 734.0M | train_time: 32.8m | wall_time: 32.8m
step    360/1335 | loss: 4.5854 | ppl: 98.0 | lr: 2.59e-04 | grad: 1.20 | tok/s: 381329 | tokens: 755.0M | train_time: 33.7m | wall_time: 33.7m
step    370/1335 | loss: 4.5488 | ppl: 94.5 | lr: 2.57e-04 | grad: 0.72 | tok/s: 368011 | tokens: 775.9M | train_time: 34.7m | wall_time: 34.7m
step    380/1335 | loss: 4.5009 | ppl: 90.1 | lr: 2.54e-04 | grad: 0.66 | tok/s: 373198 | tokens: 796.9M | train_time: 35.6m | wall_time: 35.6m
step    390/1335 | loss: 4.5338 | ppl: 93.1 | lr: 2.52e-04 | grad: 0.80 | tok/s: 380114 | tokens: 817.9M | train_time: 36.5m | wall_time: 36.5m
step    400/1335 | loss: 4.4812 | ppl: 88.3 | lr: 2.49e-04 | grad: 0.69 | tok/s: 379502 | tokens: 838.9M | train_time: 37.4m | wall_time: 37.4m
step    410/1335 | loss: 4.4790 | ppl: 88.1 | lr: 2.47e-04 | grad: 0.63 | tok/s: 367657 | tokens: 859.8M | train_time: 38.4m | wall_time: 38.4m
step    420/1335 | loss: 4.3967 | ppl: 81.2 | lr: 2.44e-04 | grad: 0.82 | tok/s: 375283 | tokens: 880.8M | train_time: 39.3m | wall_time: 39.3m
step    430/1335 | loss: 4.3743 | ppl: 79.4 | lr: 2.41e-04 | grad: 0.64 | tok/s: 371919 | tokens: 901.8M | train_time: 40.2m | wall_time: 40.2m
step    440/1335 | loss: 4.2703 | ppl: 71.5 | lr: 2.39e-04 | grad: 0.50 | tok/s: 382358 | tokens: 922.7M | train_time: 41.1m | wall_time: 41.1m
step    450/1335 | loss: 4.2641 | ppl: 71.1 | lr: 2.36e-04 | grad: 0.78 | tok/s: 367974 | tokens: 943.7M | train_time: 42.0m | wall_time: 42.0m
step    460/1335 | loss: 4.2499 | ppl: 70.1 | lr: 2.33e-04 | grad: 0.77 | tok/s: 367288 | tokens: 964.7M | train_time: 42.9m | wall_time: 42.9m
step    470/1335 | loss: 4.2404 | ppl: 69.4 | lr: 2.30e-04 | grad: 0.83 | tok/s: 376057 | tokens: 985.7M | train_time: 43.8m | wall_time: 43.8m
step    480/1335 | loss: 4.1727 | ppl: 64.9 | lr: 2.27e-04 | grad: 0.49 | tok/s: 390611 | tokens: 1006.6M | train_time: 44.8m | wall_time: 44.8m
step    490/1335 | loss: 4.2186 | ppl: 67.9 | lr: 2.25e-04 | grad: 0.90 | tok/s: 369325 | tokens: 1027.6M | train_time: 45.7m | wall_time: 45.7m
/venv/fp8/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin torch._C._distributed_c10d.PyCapsule._broadcast_coalesced. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
  → Checkpoint saved (val_loss: 4.2020)
step    500/1335 | loss: 4.1648 | val: 4.2020 | ppl: 64.4 | lr: 2.22e-04 | grad: 0.48 | tok/s: 374595 | tokens: 1048.6M | train_time: 46.6m | wall_time: 46.6m
step    510/1335 | loss: 4.1422 | ppl: 62.9 | lr: 2.19e-04 | grad: 0.89 | tok/s: 371215 | tokens: 1069.5M | train_time: 47.6m | wall_time: 48.0m
step    520/1335 | loss: 4.1225 | ppl: 61.7 | lr: 2.16e-04 | grad: 0.82 | tok/s: 363837 | tokens: 1090.5M | train_time: 48.5m | wall_time: 48.9m
step    530/1335 | loss: 4.1244 | ppl: 61.8 | lr: 2.13e-04 | grad: 0.59 | tok/s: 374785 | tokens: 1111.5M | train_time: 49.4m | wall_time: 49.9m
step    540/1335 | loss: 4.0642 | ppl: 58.2 | lr: 2.10e-04 | grad: 0.59 | tok/s: 367884 | tokens: 1132.5M | train_time: 50.4m | wall_time: 50.8m
step    550/1335 | loss: 4.0896 | ppl: 59.7 | lr: 2.07e-04 | grad: 0.80 | tok/s: 375759 | tokens: 1153.4M | train_time: 51.3m | wall_time: 51.7m
step    560/1335 | loss: 3.9458 | ppl: 51.7 | lr: 2.03e-04 | grad: 0.55 | tok/s: 369921 | tokens: 1174.4M | train_time: 52.2m | wall_time: 52.7m
step    570/1335 | loss: 4.0307 | ppl: 56.3 | lr: 2.00e-04 | grad: 0.92 | tok/s: 377153 | tokens: 1195.4M | train_time: 53.2m | wall_time: 53.6m
step    580/1335 | loss: 3.9504 | ppl: 52.0 | lr: 1.97e-04 | grad: 0.64 | tok/s: 375499 | tokens: 1216.3M | train_time: 54.1m | wall_time: 54.5m
step    590/1335 | loss: 3.9517 | ppl: 52.0 | lr: 1.94e-04 | grad: 0.54 | tok/s: 391829 | tokens: 1237.3M | train_time: 55.0m | wall_time: 55.4m
step    600/1335 | loss: 3.9438 | ppl: 51.6 | lr: 1.91e-04 | grad: 0.88 | tok/s: 388023 | tokens: 1258.3M | train_time: 55.9m | wall_time: 56.3m
step    610/1335 | loss: 3.8623 | ppl: 47.6 | lr: 1.88e-04 | grad: 0.67 | tok/s: 373095 | tokens: 1279.3M | train_time: 56.8m | wall_time: 57.3m
step    620/1335 | loss: 3.8605 | ppl: 47.5 | lr: 1.85e-04 | grad: 0.67 | tok/s: 364802 | tokens: 1300.2M | train_time: 57.8m | wall_time: 58.2m
step    630/1335 | loss: 3.8249 | ppl: 45.8 | lr: 1.81e-04 | grad: 0.53 | tok/s: 378290 | tokens: 1321.2M | train_time: 58.7m | wall_time: 59.1m
step    640/1335 | loss: 3.8799 | ppl: 48.4 | lr: 1.78e-04 | grad: 0.78 | tok/s: 376513 | tokens: 1342.2M | train_time: 59.6m | wall_time: 1.00h
step    650/1335 | loss: 3.7171 | ppl: 41.1 | lr: 1.75e-04 | grad: 0.72 | tok/s: 379864 | tokens: 1363.1M | train_time: 1.01h | wall_time: 1.02h
step    660/1335 | loss: 3.7762 | ppl: 43.7 | lr: 1.72e-04 | grad: 0.55 | tok/s: 388126 | tokens: 1384.1M | train_time: 1.02h | wall_time: 1.03h
step    670/1335 | loss: 3.8402 | ppl: 46.5 | lr: 1.68e-04 | grad: 0.76 | tok/s: 377689 | tokens: 1405.1M | train_time: 1.04h | wall_time: 1.05h
step    680/1335 | loss: 3.7615 | ppl: 43.0 | lr: 1.65e-04 | grad: 0.72 | tok/s: 382778 | tokens: 1426.1M | train_time: 1.06h | wall_time: 1.06h
step    690/1335 | loss: 3.7437 | ppl: 42.3 | lr: 1.62e-04 | grad: 0.75 | tok/s: 384558 | tokens: 1447.0M | train_time: 1.07h | wall_time: 1.08h
step    700/1335 | loss: 3.6991 | ppl: 40.4 | lr: 1.59e-04 | grad: 0.66 | tok/s: 375972 | tokens: 1468.0M | train_time: 1.09h | wall_time: 1.09h
step    710/1335 | loss: 3.6614 | ppl: 38.9 | lr: 1.55e-04 | grad: 0.89 | tok/s: 388277 | tokens: 1489.0M | train_time: 1.10h | wall_time: 1.11h
step    720/1335 | loss: 3.6855 | ppl: 39.9 | lr: 1.52e-04 | grad: 0.50 | tok/s: 382089 | tokens: 1509.9M | train_time: 1.12h | wall_time: 1.12h
step    730/1335 | loss: 3.7596 | ppl: 42.9 | lr: 1.49e-04 | grad: 1.52 | tok/s: 361291 | tokens: 1530.9M | train_time: 1.13h | wall_time: 1.14h
step    740/1335 | loss: 3.7183 | ppl: 41.2 | lr: 1.46e-04 | grad: 0.66 | tok/s: 388356 | tokens: 1551.9M | train_time: 1.15h | wall_time: 1.15h
step    750/1335 | loss: 3.6450 | ppl: 38.3 | lr: 1.43e-04 | grad: 0.48 | tok/s: 383794 | tokens: 1572.9M | train_time: 1.16h | wall_time: 1.17h
step    760/1335 | loss: 3.6956 | ppl: 40.3 | lr: 1.39e-04 | grad: 0.99 | tok/s: 392960 | tokens: 1593.8M | train_time: 1.18h | wall_time: 1.19h
step    770/1335 | loss: 3.6254 | ppl: 37.5 | lr: 1.36e-04 | grad: 0.51 | tok/s: 388462 | tokens: 1614.8M | train_time: 1.19h | wall_time: 1.20h
step    780/1335 | loss: 3.5669 | ppl: 35.4 | lr: 1.33e-04 | grad: 0.47 | tok/s: 382668 | tokens: 1635.8M | train_time: 1.21h | wall_time: 1.22h
step    790/1335 | loss: 3.5996 | ppl: 36.6 | lr: 1.30e-04 | grad: 0.72 | tok/s: 388896 | tokens: 1656.8M | train_time: 1.22h | wall_time: 1.23h
step    800/1335 | loss: 3.5531 | ppl: 34.9 | lr: 1.27e-04 | grad: 0.44 | tok/s: 379011 | tokens: 1677.7M | train_time: 1.24h | wall_time: 1.25h
step    810/1335 | loss: 3.5990 | ppl: 36.6 | lr: 1.24e-04 | grad: 0.67 | tok/s: 384348 | tokens: 1698.7M | train_time: 1.25h | wall_time: 1.26h
step    820/1335 | loss: 3.5375 | ppl: 34.4 | lr: 1.21e-04 | grad: 0.58 | tok/s: 380358 | tokens: 1719.7M | train_time: 1.27h | wall_time: 1.28h
step    830/1335 | loss: 3.5646 | ppl: 35.3 | lr: 1.18e-04 | grad: 0.65 | tok/s: 382420 | tokens: 1740.6M | train_time: 1.29h | wall_time: 1.29h
step    840/1335 | loss: 3.5801 | ppl: 35.9 | lr: 1.15e-04 | grad: 0.53 | tok/s: 382489 | tokens: 1761.6M | train_time: 1.30h | wall_time: 1.31h
step    850/1335 | loss: 3.5502 | ppl: 34.8 | lr: 1.12e-04 | grad: 0.77 | tok/s: 383494 | tokens: 1782.6M | train_time: 1.32h | wall_time: 1.32h
step    860/1335 | loss: 3.5793 | ppl: 35.8 | lr: 1.09e-04 | grad: 0.66 | tok/s: 383686 | tokens: 1803.6M | train_time: 1.33h | wall_time: 1.34h
step    870/1335 | loss: 3.6105 | ppl: 37.0 | lr: 1.06e-04 | grad: 0.49 | tok/s: 377015 | tokens: 1824.5M | train_time: 1.35h | wall_time: 1.35h
step    880/1335 | loss: 3.5619 | ppl: 35.2 | lr: 1.03e-04 | grad: 0.47 | tok/s: 372367 | tokens: 1845.5M | train_time: 1.36h | wall_time: 1.37h
step    890/1335 | loss: 3.5715 | ppl: 35.6 | lr: 9.99e-05 | grad: 0.66 | tok/s: 391878 | tokens: 1866.5M | train_time: 1.38h | wall_time: 1.38h
step    900/1335 | loss: 3.5500 | ppl: 34.8 | lr: 9.71e-05 | grad: 0.46 | tok/s: 380624 | tokens: 1887.4M | train_time: 1.39h | wall_time: 1.40h
step    910/1335 | loss: 3.5905 | ppl: 36.3 | lr: 9.43e-05 | grad: 0.46 | tok/s: 380492 | tokens: 1908.4M | train_time: 1.41h | wall_time: 1.42h
step    920/1335 | loss: 3.5186 | ppl: 33.7 | lr: 9.16e-05 | grad: 0.69 | tok/s: 388819 | tokens: 1929.4M | train_time: 1.42h | wall_time: 1.43h
step    930/1335 | loss: 3.5398 | ppl: 34.5 | lr: 8.89e-05 | grad: 0.44 | tok/s: 392881 | tokens: 1950.4M | train_time: 1.44h | wall_time: 1.45h
step    940/1335 | loss: 3.4827 | ppl: 32.5 | lr: 8.63e-05 | grad: 0.45 | tok/s: 379016 | tokens: 1971.3M | train_time: 1.45h | wall_time: 1.46h
step    950/1335 | loss: 3.5292 | ppl: 34.1 | lr: 8.36e-05 | grad: 0.58 | tok/s: 379234 | tokens: 1992.3M | train_time: 1.47h | wall_time: 1.48h
step    960/1335 | loss: 3.4410 | ppl: 31.2 | lr: 8.11e-05 | grad: 0.45 | tok/s: 386444 | tokens: 2013.3M | train_time: 1.49h | wall_time: 1.49h
step    970/1335 | loss: 3.5016 | ppl: 33.2 | lr: 7.86e-05 | grad: 0.47 | tok/s: 380975 | tokens: 2034.2M | train_time: 1.50h | wall_time: 1.51h
step    980/1335 | loss: 3.4845 | ppl: 32.6 | lr: 7.61e-05 | grad: 0.58 | tok/s: 379077 | tokens: 2055.2M | train_time: 1.52h | wall_time: 1.52h
step    990/1335 | loss: 3.4553 | ppl: 31.7 | lr: 7.37e-05 | grad: 0.50 | tok/s: 376995 | tokens: 2076.2M | train_time: 1.53h | wall_time: 1.54h
  → Checkpoint saved (val_loss: 3.5034)
step   1000/1335 | loss: 3.4181 | val: 3.5034 | ppl: 30.5 | lr: 7.13e-05 | grad: 0.37 | tok/s: 377920 | tokens: 2097.2M | train_time: 1.55h | wall_time: 1.55h
step   1010/1335 | loss: 3.5032 | ppl: 33.2 | lr: 6.90e-05 | grad: 0.48 | tok/s: 374465 | tokens: 2118.1M | train_time: 1.56h | wall_time: 1.57h
step   1020/1335 | loss: 3.4168 | ppl: 30.5 | lr: 6.68e-05 | grad: 0.45 | tok/s: 383964 | tokens: 2139.1M | train_time: 1.58h | wall_time: 1.59h
step   1030/1335 | loss: 3.4493 | ppl: 31.5 | lr: 6.46e-05 | grad: 0.54 | tok/s: 377019 | tokens: 2160.1M | train_time: 1.59h | wall_time: 1.60h
step   1040/1335 | loss: 3.4586 | ppl: 31.8 | lr: 6.24e-05 | grad: 0.41 | tok/s: 389987 | tokens: 2181.0M | train_time: 1.61h | wall_time: 1.62h
step   1050/1335 | loss: 3.5158 | ppl: 33.6 | lr: 6.04e-05 | grad: 0.40 | tok/s: 378496 | tokens: 2202.0M | train_time: 1.63h | wall_time: 1.63h
step   1060/1335 | loss: 3.4636 | ppl: 31.9 | lr: 5.84e-05 | grad: 0.44 | tok/s: 378756 | tokens: 2223.0M | train_time: 1.64h | wall_time: 1.65h
step   1070/1335 | loss: 3.4801 | ppl: 32.5 | lr: 5.64e-05 | grad: 0.44 | tok/s: 369989 | tokens: 2244.0M | train_time: 1.66h | wall_time: 1.67h
step   1080/1335 | loss: 3.5197 | ppl: 33.8 | lr: 5.45e-05 | grad: 0.47 | tok/s: 361627 | tokens: 2264.9M | train_time: 1.67h | wall_time: 1.68h
step   1090/1335 | loss: 3.4712 | ppl: 32.2 | lr: 5.27e-05 | grad: 0.39 | tok/s: 359649 | tokens: 2285.9M | train_time: 1.69h | wall_time: 1.70h
step   1100/1335 | loss: 3.4568 | ppl: 31.7 | lr: 5.09e-05 | grad: 0.44 | tok/s: 355233 | tokens: 2306.9M | train_time: 1.70h | wall_time: 1.71h
step   1110/1335 | loss: 3.4647 | ppl: 32.0 | lr: 4.92e-05 | grad: 0.37 | tok/s: 388085 | tokens: 2327.8M | train_time: 1.72h | wall_time: 1.73h
step   1120/1335 | loss: 3.3854 | ppl: 29.5 | lr: 4.76e-05 | grad: 0.43 | tok/s: 380167 | tokens: 2348.8M | train_time: 1.73h | wall_time: 1.74h
step   1130/1335 | loss: 3.4367 | ppl: 31.1 | lr: 4.60e-05 | grad: 0.37 | tok/s: 375456 | tokens: 2369.8M | train_time: 1.75h | wall_time: 1.76h
step   1140/1335 | loss: 3.4350 | ppl: 31.0 | lr: 4.45e-05 | grad: 0.47 | tok/s: 376837 | tokens: 2390.8M | train_time: 1.77h | wall_time: 1.77h
step   1150/1335 | loss: 3.3477 | ppl: 28.4 | lr: 4.31e-05 | grad: 0.43 | tok/s: 368719 | tokens: 2411.7M | train_time: 1.78h | wall_time: 1.79h
step   1160/1335 | loss: 3.3551 | ppl: 28.6 | lr: 4.17e-05 | grad: 0.33 | tok/s: 378026 | tokens: 2432.7M | train_time: 1.80h | wall_time: 1.81h
step   1170/1335 | loss: 3.4464 | ppl: 31.4 | lr: 4.04e-05 | grad: 0.38 | tok/s: 351224 | tokens: 2453.7M | train_time: 1.81h | wall_time: 1.82h
step   1180/1335 | loss: 3.3304 | ppl: 27.9 | lr: 3.92e-05 | grad: 0.38 | tok/s: 372616 | tokens: 2474.6M | train_time: 1.83h | wall_time: 1.84h
step   1190/1335 | loss: 3.3941 | ppl: 29.8 | lr: 3.81e-05 | grad: 0.38 | tok/s: 377665 | tokens: 2495.6M | train_time: 1.84h | wall_time: 1.85h
step   1200/1335 | loss: 3.4934 | ppl: 32.9 | lr: 3.70e-05 | grad: 0.41 | tok/s: 368606 | tokens: 2516.6M | train_time: 1.86h | wall_time: 1.87h
step   1210/1335 | loss: 3.4672 | ppl: 32.0 | lr: 3.60e-05 | grad: 0.34 | tok/s: 386501 | tokens: 2537.6M | train_time: 1.87h | wall_time: 1.88h
step   1220/1335 | loss: 3.4655 | ppl: 32.0 | lr: 3.51e-05 | grad: 0.34 | tok/s: 371466 | tokens: 2558.5M | train_time: 1.89h | wall_time: 1.90h
step   1230/1335 | loss: 3.4644 | ppl: 32.0 | lr: 3.43e-05 | grad: 0.34 | tok/s: 368432 | tokens: 2579.5M | train_time: 1.90h | wall_time: 1.91h
step   1240/1335 | loss: 3.5126 | ppl: 33.5 | lr: 3.35e-05 | grad: 0.39 | tok/s: 369992 | tokens: 2600.5M | train_time: 1.92h | wall_time: 1.93h
step   1250/1335 | loss: 3.4406 | ppl: 31.2 | lr: 3.28e-05 | grad: 0.37 | tok/s: 388486 | tokens: 2621.4M | train_time: 1.94h | wall_time: 1.94h
step   1260/1335 | loss: 3.4479 | ppl: 31.4 | lr: 3.22e-05 | grad: 0.35 | tok/s: 371059 | tokens: 2642.4M | train_time: 1.95h | wall_time: 1.96h
step   1270/1335 | loss: 3.3884 | ppl: 29.6 | lr: 3.16e-05 | grad: 0.37 | tok/s: 374063 | tokens: 2663.4M | train_time: 1.97h | wall_time: 1.98h
step   1280/1335 | loss: 3.3704 | ppl: 29.1 | lr: 3.12e-05 | grad: 0.34 | tok/s: 381896 | tokens: 2684.4M | train_time: 1.98h | wall_time: 1.99h
step   1290/1335 | loss: 3.4177 | ppl: 30.5 | lr: 3.08e-05 | grad: 0.37 | tok/s: 387904 | tokens: 2705.3M | train_time: 2.00h | wall_time: 2.01h
step   1300/1335 | loss: 3.3793 | ppl: 29.3 | lr: 3.05e-05 | grad: 0.34 | tok/s: 388203 | tokens: 2726.3M | train_time: 2.01h | wall_time: 2.02h
step   1310/1335 | loss: 3.3594 | ppl: 28.8 | lr: 3.02e-05 | grad: 0.37 | tok/s: 393531 | tokens: 2747.3M | train_time: 2.03h | wall_time: 2.04h
step   1320/1335 | loss: 3.4078 | ppl: 30.2 | lr: 3.01e-05 | grad: 0.35 | tok/s: 388768 | tokens: 2768.2M | train_time: 2.04h | wall_time: 2.05h
step   1330/1335 | loss: 3.4402 | ppl: 31.2 | lr: 3.00e-05 | grad: 0.36 | tok/s: 390807 | tokens: 2789.2M | train_time: 2.06h | wall_time: 2.07h

-----------------Final Validation-----------------
Final validation loss: 3.4399
Final validation perplexity: 31.18

======================================================================
TRAINING COMPLETE
======================================================================
GPUs: 1
Total steps: 1335
Total tokens: 2799.7M (2.80B)
Training time: 2.07h
Wall time: 2.08h
Average throughput: 376307 tokens/sec
Per-GPU throughput: 376307 tokens/sec/GPU
Final train loss: 3.4303
Final val loss: 3.4399
Best val loss: 3.5034
Mode: fp8_tensorwise
Log saved to: experiments/125M_fp8_20260107_110339/train_fp8_125M.csv
Best checkpoint saved to: experiments/125M_fp8_20260107_110339/checkpoint_fp8_125M.pt
======================================================================
